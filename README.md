# Pipeline de dados: combinando Python e Orienta√ß√£o a Objeto

## üí°Objetivos
Reposit√≥rio dedicado a contribuir para o desenvolvimento de um Pipeline de Dados abrangendo as fases de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento de Dados). Este pipeline √© projetado para oferecer flexibilidade e facilidade de reutiliza√ß√£o em diversas situa√ß√µes.

## üñ•Ô∏èDesafios do Projeto
Duas empresas recentemente se uniram, cada uma com suas pr√≥prias bases de dados, apresentando origens e estruturas distintas. No √¢mbito do nosso time de engenharia de dados, a responsabilidade recai sobre a integra√ß√£o dessas duas fontes de dados, realizando as transforma√ß√µes necess√°rias e estabelecendo um pipeline robusto. Esse pipeline visa garantir a reprodu√ß√£o eficiente do processo nos meses subsequentes, quando a demanda por essa integra√ß√£o ressurgir.

O objetivo final √© disponibilizar esses dados consolidados para a equipe de Business Intelligence (BI), capacitando-os a criar relat√≥rios detalhados, extrair insights valiosos e compreender os resultados decorrentes dessa fus√£o empresarial. Essas an√°lises buscar√° responder quest√µes cruciais, se as vendas aumentaram, diminu√≠ram ou se concentraram predominantemente na empresa A ou B.

## üìÑAtividades Realizadas
|Atividades|Realizadas |
|----------|-----------|
|Estruturar um projeto de engenharia de dados|Mudar o comportamento do m√©todo get do tipo dicion√°rio|
|Comandos Linux de cria√ß√£o de pastas|Utilizar o for e o get para construir uma nova estrutura de lista de listas|
|Baixar dados utilizando wget|Salvar os dados com writer do pacote csv|
|Leitura de dados com o comando open|Sair da etapa de explora√ß√£o para a etapa de cria√ß√£o de um pipeline|
|Biblioteca csv e json para leitura eficiente dos dados|Refatorar c√≥digo criando fun√ß√µes para a extra√ß√£o, transforma√ß√£o e carregamento|
|Escolher uma estrutura para armazenar seus dados|Gerar um relat√≥rio do pipeline informando os dados lidos e gerados|
|Utilizar as fun√ß√µes DictReader e readlines|Identificar as etapas do pipeline ETL no c√≥digo|
|Utilizar o m√©todo keys, get e items do tipo dicion√°rio|Identificar a necessidade de aplicar o paradigma orienta√ß√£o a objeto|
|Alinhar decis√µes com o cliente|Transformar um pipeline de dados de fun√ß√µes para uma classe|
|Combinar o for com as fun√ß√µes do dicion√°rio renomear as colunas|Criar os atributos e m√©todos da classe Dados|
|Utilizando o m√©todo extend do tipo lista para juntar nossos dados|Refatorar c√≥digo para novas necessidades|
|Salvando os dados com DictWriter|Utilizar um pipeline de Dados em script|

##  üóÇÔ∏èEstrutura de Pastas
- data_raw: Diret√≥rio que cont√©m os dados no formato bruto, antes de qualquer processamento.
- data_processed: Armazenamento dedicado aos dados ap√≥s passarem por transforma√ß√µes e an√°lises, representando a vers√£o trabalhada e refinada.
- notebook: Pasta destinada aos arquivos Jupyter Notebook, nos quais s√£o realizadas an√°lises e explora√ß√µes aprofundadas dos dados.
- scripts: Espa√ßo designado para os scripts em Python, encapsulando o c√≥digo utilizado no processo de manipula√ß√£o e transforma√ß√£o dos dados.

## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)